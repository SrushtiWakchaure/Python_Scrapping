{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_product_list(url, num_pages):\n",
        "    base_url = url\n",
        "    headers = {\"User-Agent\": \"Your User-Agent String\"}\n",
        "\n",
        "    with open('amazon_products.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "\n",
        "        for page in range(1, num_pages + 1):\n",
        "            page_url = f\"{base_url}&page={page}\"\n",
        "            response = requests.get(page_url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            products = soup.find_all('div', {'data-asin': True, 'data-component-type': 's-search-result'})\n",
        "\n",
        "            for product in products:\n",
        "                product_url = f\"https://www.amazon.in{product.find('a', {'class': 'a-link-normal'})['href']}\"\n",
        "                product_name = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
        "\n",
        "                price_element = product.find('span', {'class': 'a-offscreen'})\n",
        "                product_price = price_element.text.strip() if price_element else \"N/A\"\n",
        "\n",
        "                rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
        "                product_rating = rating_element.text.split()[0] if rating_element else \"N/A\"\n",
        "\n",
        "                num_reviews_element = product.find('span', {'class': 'a-size-base', 'dir': 'auto'})\n",
        "                num_reviews = num_reviews_element.text.strip() if num_reviews_element else \"N/A\"\n",
        "\n",
        "                writer.writerow([product_url, product_name, product_price, product_rating, num_reviews])\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(f\"Scraping completed for {num_pages} pages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
        "    num_pages_to_scrape = 20\n",
        "    scrape_product_list(url, num_pages_to_scrape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FvoEWLXk_pJ",
        "outputId": "fc7e49b3-bc8d-4d44-efcc-d3c5cac0f948"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed for 20 pages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def scrape_product_details(product_urls):\n",
        "    headers = {\"User-Agent\": \"Your User-Agent String\"}\n",
        "\n",
        "    with open('product_details.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Product URL', 'Description', 'ASIN', 'Product Description', 'Manufacturer'])\n",
        "\n",
        "        for product_url in product_urls:\n",
        "            response = requests.get(product_url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(\"Scraping completed for product details.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    product_urls = []\n",
        "\n",
        "    scrape_product_details(product_urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbm4TGjPlZ2N",
        "outputId": "dcafd7ac-8d53-4379-b8bd-93f68bcf26ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed for product details.\n"
          ]
        }
      ]
    }
  ]
}